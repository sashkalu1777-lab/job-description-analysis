{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81a4267",
   "metadata": {},
   "source": [
    "# Baseline without RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c1bf4",
   "metadata": {},
   "source": [
    "Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3438b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from json import loads, dumps\n",
    "from langchain.tools import tool\n",
    "# from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# from pydantic import BaseModel, Field\n",
    "from typing import Annotated, Sequence, TypedDict, Literal\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langfuse import Langfuse, get_client\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from langfuse.openai import openai  # Don't delete this line.\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import tools_condition, ToolNode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222df19d",
   "metadata": {},
   "source": [
    "Set up environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696f7d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n"
     ]
    }
   ],
   "source": [
    "# Set up environment variables.\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Monitor if the environment variables are ready.\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set - please head to the troubleshooting guide in the setup folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5736d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse client is authenticated and ready!\n"
     ]
    }
   ],
   "source": [
    "langfuse_secret_key = os.getenv('LANGFUSE_SECRET_KEY')\n",
    "langfuse_public_key = os.getenv('LANGFUSE_PUBLIC_KEY')\n",
    "langfuse = Langfuse(\n",
    "    secret_key = langfuse_secret_key,\n",
    "    public_key = langfuse_public_key,\n",
    "    host=\"https://cloud.langfuse.com\"\n",
    "    # host = \"https://us.cloud.langfuse.com\"\n",
    ")\n",
    "\n",
    "langfuse = get_client()\n",
    " \n",
    "# Verify connection\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse client is authenticated and ready!\")\n",
    "else:\n",
    "    print(\"Authentication failed. Please check your credentials and host.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a89de",
   "metadata": {},
   "source": [
    "Define supporting variables and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a2f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # \"vector_store_directory\": \"./chroma_db\",\n",
    "    # \"vector_store_collection_name\": \"langchain_rag\",\n",
    "    # \"chunk_size\": 500,\n",
    "    # \"chunk_overlap\": 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d26637a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_job_descriptions(jd_dataframe):\n",
    "    if isinstance(jd_dataframe, pd.Series):\n",
    "        jd_dataframe = jd_dataframe.to_frame().T\n",
    "    result = jd_dataframe.to_json(orient='records')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6542a8e",
   "metadata": {},
   "source": [
    "Function test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b4ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/X_test.csv\")\n",
    "test = df.iloc[0:2, :]\n",
    "if isinstance(test, pd.Series):\n",
    "        test = test.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21b4bb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"title\": \"Driver / Tour Guide for Coach & 4WD Coach Tours\",\n",
      "        \"abstract\": \"This role includes conducting 1-20 Day Tours to a range of locations all over Australia with mature passengers for a well established family company.\",\n",
      "        \"job_description_clean\": \"Casey Australia Tours require a experienced Coach Drivers / Tour Guide for the rest of the 2019 season of extended motel and/or camping tours starting mid/late June The role requires driving, minor coach maintenance, cleaning, guiding and commentating plus some cooking. The ideal person should have; 1. A sense of humour and engaging personality 2. Initiative and problem solving skills 3. Confidence and be a good public speaker 4. A current manual HR drivers licence with F endorsement. (or ability to get one quickly) 5. A safe driving record 6. Clear Criminal History 7. Experience in some field that can be related 8. First Aid Training 9. Good Navigation in remote and city areas Handy assets to have; 1. Mechanical Knowledge 2. Camping experience 3. Group Cooking Experience Our passengers are mainly Australian adults from 50 - 80+ years old. Each tour is run by 1 - 2 Staff caring for up to 48 passengers. It is essential that staff cooperate well and work closely together. This is an interesting, exciting, varied and extremely rewarding role. It's an enjoyable lifestyle with its share of hard days but also has a lot of fun easy days in some stunning locations. To view our range of tours visit www.caseytours.com.au/calendar/ To get a better idea of the places, people and coaches go to our Facebook page, www.facebook.com/CaseyAustraliaTours Rate of Pay $300 - $350 Per day (depending on experience and knowledge) + Super Work is on a tour by tour basis from mid /late June, scheduling is roughly two weeks on and one off but varies. The season goes through to November with ongoing employment for available. For the right applicant the role could be based in nearly any capital city flying in and out to different towns to start and finish tours. If you feel you are up to the challenge, please: 1. Reply to this ad on Seek 2. Send a CV and Cover Letter by email to tcasey@caseytours.com.au\",\n",
      "        \"classification\": \"Hospitality & Tourism\",\n",
      "        \"subClassification\": \"Tour Guides\",\n",
      "        \"area\": null,\n",
      "        \"location\": \"Perth\",\n",
      "        \"suburb\": null,\n",
      "        \"workType\": \"Casual/Vacation\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Storepersons Required\",\n",
      "        \"abstract\": \"Our client is currently looking for an enthusiastic and eager Storeperson to join their team in Rockdale.\",\n",
      "        \"job_description_clean\": \"Our client is currently seeking an experienced store person/s to assist their team in Rockdale. The company is a market leader in supply to fabricators and distributors, focusing on the Residential, Commercial and Industrial segments. This position is Monday - Friday, 8am - 4pm. This position will start immediate as soon as the perfect candidate is found. Your role will entail the following duties. Liaising with customer at the front desk of the warehouse. Answering phone calls and emails in a professional manner and provide prices and take down details accurately. Multitask with ease and work in a busy team environment Follow warehouse processes and procedures General pick packing duties. Instructing staff members to gather products for customer orders. Your Skills: Excellent time management and the ability to prioritise tasks Fast and accurate typing skills Coordinating the flow of timely information to and from our customers Developing relationships with our drivers and key customers A positive energy Efficient and reliable Forklift ticket desired but not essential. If you believe this job description suits your skills and desires, please apply!\",\n",
      "        \"classification\": \"Manufacturing, Transport & Logistics\",\n",
      "        \"subClassification\": \"Warehousing, Storage & Distribution\",\n",
      "        \"area\": \"Southern Suburbs & Sutherland Shire\",\n",
      "        \"location\": \"Sydney\",\n",
      "        \"suburb\": \"Rockdale\",\n",
      "        \"workType\": \"Full Time\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "json_records = test.to_json(orient='records')\n",
    "parsed = loads(json_records)\n",
    "print(dumps(parsed, indent=4) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96280d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def check_basic_information(job_descriptions) -> dict:\n",
    "    \"\"\"\n",
    "    This function consumes job_descriptions and return basic information with good formatted strings.\n",
    "    \"\"\"\n",
    "    if isinstance(job_descriptions, pd.Series):\n",
    "        job_descriptions = job_descriptions.to_frame().T\n",
    "\n",
    "    list_of_dicts = job_descriptions.to_dict(orient='records')\n",
    "    \n",
    "    jobs_info = []\n",
    "    for i, job_dict in enumerate(list_of_dicts):\n",
    "        jobs_info.append({\n",
    "            \"job_index\": i,\n",
    "            \"fields\": job_dict\n",
    "        })\n",
    "\n",
    "    result = {\n",
    "        \"summary\": \"The information of each job advertisement is as follows.\",\n",
    "        \"jobs\": jobs_info\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "tools = [check_basic_information]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f70729",
   "metadata": {},
   "source": [
    "Define Graph Sate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0089e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of graph.\n",
    "\n",
    "    Attributes:\n",
    "        messages: A list of messages in the conversation, including user input and agent outputs.\n",
    "\n",
    "    Notes:\n",
    "        The add_messages function defines how an update should be processed.\n",
    "        Default is to replace. add_messages says \"append\".\n",
    "    \"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf385b",
   "metadata": {},
   "source": [
    "Deinfe Basic Nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de3a90f",
   "metadata": {},
   "source": [
    "Input: job description strings.\n",
    "\n",
    "Host node\n",
    "\n",
    "Conditional Edge: whether the question is about to check job descriptions.\n",
    "\n",
    "If not, return generate.\n",
    "\n",
    "If yes, go to next node.\n",
    "\n",
    "Get industry information and professional skills.\n",
    "\n",
    "Get general requirements.\n",
    "\n",
    "Evaluate the job advertisement.\n",
    "\n",
    "Refine the job advertisement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff364ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def host(state):\n",
    "    \"\"\"\n",
    "    Invokes the whole workflow and monitor the state.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4-turbo\")\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def industry_agent(state, config):\n",
    "    \"\"\"\n",
    "    Generate answer. The core of RAG.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = config[\"configurable\"][\"questions\"][-1]\n",
    "    print(\"Original question:\\n\", question)\n",
    "    \n",
    "    # This is because Retriver will be in front of generate.\n",
    "    last_message = messages[-1]\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "            You are an assistant for question-answering tasks. \\n\n",
    "            Use the following pieces of retrieved context to answer the question. \\n\n",
    "            If you don't know the answer, just say that you don't know. \\n\n",
    "            Use three sentences maximum and keep the answer concise. \\n\n",
    "            Question: {question} \\n\n",
    "            Context: {context} \\n\n",
    "            Answer: \\n \n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0, streaming=True)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_agent(state, config):\n",
    "    \"\"\"\n",
    "    Generate answer. The core of RAG.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = config[\"configurable\"][\"questions\"][-1]\n",
    "    print(\"Original question:\\n\", question)\n",
    "    \n",
    "    # This is because Retriver will be in front of generate.\n",
    "    last_message = messages[-1]\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "            You are an assistant for question-answering tasks. \\n\n",
    "            Use the following pieces of retrieved context to answer the question. \\n\n",
    "            If you don't know the answer, just say that you don't know. \\n\n",
    "            Use three sentences maximum and keep the answer concise. \\n\n",
    "            Question: {question} \\n\n",
    "            Context: {context} \\n\n",
    "            Answer: \\n \n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0, streaming=True)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa6f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(state, config):\n",
    "    \"\"\"\n",
    "    Generate answer. The core of RAG.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = config[\"configurable\"][\"questions\"][-1]\n",
    "    print(\"Original question:\\n\", question)\n",
    "    \n",
    "    # This is because Retriver will be in front of generate.\n",
    "    last_message = messages[-1]\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "            You are an assistant for question-answering tasks. \\n\n",
    "            Use the following pieces of retrieved context to answer the question. \\n\n",
    "            If you don't know the answer, just say that you don't know. \\n\n",
    "            Use three sentences maximum and keep the answer concise. \\n\n",
    "            Question: {question} \\n\n",
    "            Context: {context} \\n\n",
    "            Answer: \\n \n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0, streaming=True)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362637a",
   "metadata": {},
   "source": [
    "Define conditional edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c4c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edges\n",
    "def check_task(state, config) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Pydantic to ensure the output format.\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o\", streaming=True)\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "            You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "            Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "            Here is the user question: {question} \\n\n",
    "            If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "            Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    # Because retriever is always in front of grading.\n",
    "    last_message = messages[-1]\n",
    "    docs = last_message.content\n",
    "    \n",
    "    question = config[\"configurable\"][\"questions\"][-1]\n",
    "    print(\"Original question:\\n\", question)\n",
    "    \n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        return \"rewrite\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfbc4e0",
   "metadata": {},
   "source": [
    "Complie the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes.\n",
    "workflow.add_node(\"agent\", agent)  \n",
    "\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)  \n",
    "workflow.add_node(\"rewrite\", rewrite)  \n",
    "workflow.add_node(\"generate\", generate)  \n",
    "\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile(checkpointer=MemorySaver())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b548aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
