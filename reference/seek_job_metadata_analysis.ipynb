{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEEK Job Metadata Extraction \u2014 Case Study\n",
        "This notebook implements a hybrid pipeline combining **traditional NLP** and **Generative AI** techniques to extract structured information (skills, responsibilities, requirements) from SEEK job advertisements.\n",
        "\n",
        "**Author:** ChatGPT (generated for Xiaoshi Lu)\n",
        "\n",
        "## Overview\n",
        "- Load and inspect SEEK job ad dataset\n",
        "- Extract and clean text fields\n",
        "- Apply NLP keyword extraction (KeyBERT, heuristics)\n",
        "- Apply Generative AI structured extraction (LLM)\n",
        "- Evaluate, summarize and export results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries and define functions\n",
        "# (Content copied from seek_job_metadata_analysis.py)\n",
        "\"\"\"\nseek_job_metadata_analysis.py\n\nHybrid pipeline for extracting structured metadata (skills, responsibilities, requirements)\nfrom SEEK job ads (each row JSON in a CSV).\n\nDesigned for local execution with optional LLM API integration (OpenAI-compatible).\nProduces:\n - structured JSON output per ad\n - evaluation utilities (manual labels or sampling)\n - simple PowerPoint export for presentation\n\nUsage:\n    1. Install required Python packages (examples provided below).\n    2. Place your `random_rows.csv` in the same folder or pass the path.\n    3. Export OPENAI_API_KEY in your environment if using OpenAI LLM calls.\n    4. Run: python seek_job_metadata_analysis.py --input /mnt/data/random_rows.csv --output ./results.jsonl\n\nRequirements (suggested):\n    pip install -U spacy pandas tqdm python-dotenv sentence-transformers keybert openai python-pptx rake-nltk nltk\n    python -m spacy download en_core_web_sm\n\nNotes:\n - LLM usage is optional. The script contains a `llm_extract` function that calls OpenAI's chat completions\n   if an API key is present. You can adapt it to other providers by replacing `call_openai_llm`.\n - The code is modular: replace or extend extraction methods as desired.\n\nAuthor: ChatGPT (generated)\nDate: 2025\n\"\"\"\n\nimport os\nimport json\nimport argparse\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional, Tuple\nimport re\nimport logging\nfrom tqdm import tqdm\n\n# --- Optional heavy imports guarded with try/except for graceful degradation ---\ntry:\n    import pandas as pd\nexcept Exception:\n    pd = None\n\ntry:\n    import spacy\n    from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\nexcept Exception:\n    spacy = None\n\ntry:\n    from sentence_transformers import SentenceTransformer, util as sbert_util\nexcept Exception:\n    SentenceTransformer = None\n    sbert_util = None\n\ntry:\n    from keybert import KeyBERT\nexcept Exception:\n    KeyBERT = None\n\ntry:\n    import openai\nexcept Exception:\n    openai = None\n\ntry:\n    from pptx import Presentation\n    from pptx.util import Inches, Pt\nexcept Exception:\n    Presentation = None\n\n# Fallback simple keyword extraction using regex and stopwords\nimport math\nfrom collections import Counter, defaultdict\n\n# --- Logging ---\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\nlogger = logging.getLogger(\"seek-extract\")\n\n# -------------------------\n# Utilities and preprocessing\n# -------------------------\ndef load_jsonl_csv(filepath: str, json_column: Optional[str] = None) -> List[Dict[str, Any]]:\n    \"\"\"\n    Load data from CSV where each row contains a JSON object (string) or\n    where the CSV columns are flattened. This function attempts to:\n      - If a CSV cell contains a JSON string, parse each row's first column as JSON\n      - Else, load entire CSV with pandas and convert rows to dicts\n\n    Returns list of dicts (job ads).\n    \"\"\"\n    path = Path(filepath)\n    if not path.exists():\n        raise FileNotFoundError(filepath)\n    # Try reading as text lines: each line is JSON\n    try:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            sample = f.read(1000)\n            f.seek(0)\n            # If file looks like JSON lines (starts with { or [)\n            if sample.lstrip().startswith(\"{\"):\n                objs = []\n                f.seek(0)\n                for line in f:\n                    line = line.strip()\n                    if not line:\n                        continue\n                    try:\n                        objs.append(json.loads(line))\n                    except json.JSONDecodeError:\n                        # Maybe CSV with quoted JSON in one column; try split\n                        try:\n                            # take everything after the first comma as JSON\n                            parts = line.split(\",\", 1)\n                            candidate = parts[-1]\n                            objs.append(json.loads(candidate))\n                        except Exception:\n                            continue\n                if objs:\n                    logger.info(f\"Loaded {len(objs)} JSON-lines entries from {filepath}\")\n                    return objs\n    except Exception:\n        pass\n\n    # Fallback: try pandas\n    if pd is None:\n        raise RuntimeError(\"pandas not available and file is not JSON-lines. Install pandas or provide JSON-lines file.\")\n    df = pd.read_csv(path, dtype=str, keep_default_na=False, na_filter=False)\n    logger.info(f\"CSV loaded with shape {df.shape}\")\n    # If a json_column is provided, parse it\n    if json_column and json_column in df.columns:\n        records = []\n        for val in df[json_column].astype(str).tolist():\n            try:\n                records.append(json.loads(val))\n            except Exception:\n                records.append({\"raw_text\": val})\n        return records\n    # Otherwise, convert each row to dict\n    records = df.replace({pd.NA: None}).to_dict(orient=\"records\")\n    return records\n\ndef basic_clean_text(text: str) -> str:\n    if not text:\n        return \"\"\n    # Remove HTML tags\n    text = re.sub(r\"<[^>]+>\", \" \", text)\n    # Replace escape sequences\n    text = text.replace(\"\\\\n\", \" \").replace(\"\\\\t\", \" \")\n    # Normalize whitespace\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n# -------------------------\n# Baseline: simple keyword extraction (RAKE-like)\n# -------------------------\ndef simple_keyword_candidates(text: str, stopwords: Optional[set] = None, max_ngram: int = 3) -> List[str]:\n    \"\"\"\n    A simple candidate extractor: returns phrases that exclude stopwords and punctuation.\n    \"\"\"\n    if not text:\n        return []\n    if stopwords is None:\n        stopwords = {\"and\",\"or\",\"the\",\"a\",\"an\",\"with\",\"to\",\"of\",\"in\",\"for\",\"on\",\"at\",\"by\",\"from\",\"as\",\"that\",\"is\",\"are\",\"be\",\"will\",\"experience\",\"experience.\"}\n    # Lowercase and remove non-alphanum except spaces\n    cleaned = re.sub(r\"[^A-Za-z0-9\\s/+\\-]\", \" \", text.lower())\n    tokens = cleaned.split()\n    candidates = []\n    n = len(tokens)\n    for i in range(n):\n        for L in range(1, max_ngram+1):\n            if i+L <= n:\n                phrase = \" \".join(tokens[i:i+L])\n                # Skip phrases that are only stopwords\n                if any(t in stopwords for t in phrase.split()):\n                    continue\n                if len(phrase) < 2:\n                    continue\n                candidates.append(phrase)\n    # Score by frequency\n    counts = Counter(candidates)\n    common = [kw for kw, _ in counts.most_common(30)]\n    return common\n\n# -------------------------\n# KeyBERT wrapper (if available)\n# -------------------------\ndef keybert_extract(text: str, top_n: int = 10, model_name: str = \"all-MiniLM-L6-v2\") -> List[str]:\n    if KeyBERT is None or SentenceTransformer is None:\n        return []\n    try:\n        sbert = SentenceTransformer(model_name)\n        kw_model = KeyBERT(sbert)\n        keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1,3), stop_words=\"english\", top_n=top_n)\n        return [k[0] for k in keywords]\n    except Exception as e:\n        logger.warning(\"KeyBERT extract failed: %s\", e)\n        return []\n\n# -------------------------\n# Embedding helpers\n# -------------------------\ndef load_sbert_model(model_name: str = \"all-MiniLM-L6-v2\"):\n    if SentenceTransformer is None:\n        raise RuntimeError(\"sentence-transformers not installed.\")\n    return SentenceTransformer(model_name)\n\ndef embed_texts(model, texts: List[str]):\n    if model is None:\n        raise RuntimeError(\"Embedding model not loaded.\")\n    return model.encode(texts, convert_to_tensor=True, show_progress_bar=False)\n\n# -------------------------\n# LLM extraction (OpenAI-compatible)\n# -------------------------\nDEFAULT_PROMPT = \"\"\"\nYou are a helpful assistant that extracts structured information from job advertisements.\n\nInput job ad:\n{job_text}\n\nInstruction:\nExtract the following fields from the job ad. If a field is not present, return an empty list for that field.\n\n- skills: short list of skills, tools, technologies (e.g., \"Python\", \"SQL\", \"scikit-learn\", \"communication\")\n- responsibilities: short concise list of responsibilities / duties described in the ad\n- requirements: candidate requirements such as experience, qualifications, degrees, certifications, years of experience\n\nReturn a JSON object ONLY with keys: \"skills\", \"responsibilities\", \"requirements\". Example:\n{{\"skills\": [\"Python\", \"SQL\"], \"responsibilities\": [\"build predictive models\"], \"requirements\": [\"3+ years experience\", \"Bachelor's degree\"]}}\n\"\"\"\n\ndef call_openai_llm(system_prompt: str, user_prompt: str, api_key: Optional[str] = None, model: str = \"gpt-4o-mini\"):\n    \"\"\"\n    Minimal wrapper for OpenAI ChatCompletion (may need adaptation to your OpenAI client version).\n    Replace with your own provider wrapper if needed.\n    \"\"\"\n    if openai is None:\n        raise RuntimeError(\"openai package not installed or available.\")\n    if api_key is None:\n        api_key = os.environ.get(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise RuntimeError(\"No OpenAI API key found in environment or arguments.\")\n    openai.api_key = api_key\n    # Using ChatCompletion (this code may require adaptation depending on openai package version)\n    try:\n        response = openai.ChatCompletion.create(\n            model=model,\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n            temperature=0.0,\n            max_tokens=1000\n        )\n        text = response[\"choices\"][0][\"message\"][\"content\"]\n        return text\n    except Exception as e:\n        logger.error(\"OpenAI call failed: %s\", e)\n        raise\n\ndef llm_extract(job_text: str, model: str = \"gpt-4o-mini\", api_key: Optional[str] = None, prompt_template: str = DEFAULT_PROMPT, safe_mode: bool = True) -> Dict[str, List[str]]:\n    \"\"\"\n    Extract structured fields via LLM. Returns dict with lists.\n    \"\"\"\n    prompt = prompt_template.format(job_text=job_text)\n    raw = call_openai_llm(system_prompt=\"You are a JSON-output assistant.\", user_prompt=prompt, api_key=api_key, model=model)\n    # Attempt to parse JSON from LLM output\n    parsed = {\"skills\": [], \"responsibilities\": [], \"requirements\": []}\n    try:\n        # Try to find the first JSON object in the text\n        start = raw.find(\"{\")\n        end = raw.rfind(\"}\")\n        if start != -1 and end != -1:\n            candidate = raw[start:end+1]\n            parsed_candidate = json.loads(candidate)\n            # Ensure keys exist\n            for k in parsed.keys():\n                if k in parsed_candidate and isinstance(parsed_candidate[k], list):\n                    parsed[k] = [str(x).strip() for x in parsed_candidate[k] if x]\n    except Exception as e:\n        logger.warning(\"Failed to parse LLM JSON output: %s. Raw output: %s\", e, raw[:500])\n        # As fallback, do naive regex extraction of lines starting with \"-\"\n        lines = raw.splitlines()\n        for line in lines:\n            m = re.match(r\"skills[:\\-]\\s*(.*)\", line, re.I)\n            if m:\n                parsed[\"skills\"] = [x.strip() for x in re.split(r\",|\\;\", m.group(1)) if x.strip()]\n    return parsed\n\n# -------------------------\n# High-level pipeline\n# -------------------------\ndef extract_from_ad(ad: Dict[str, Any], sbert_model=None, use_keybert: bool = True, llm_api_key: Optional[str] = None, llm_model: str = \"gpt-4o-mini\") -> Dict[str, Any]:\n    \"\"\"\n    Extract metadata from a single job ad dict. Returns structured dict.\n    \"\"\"\n    title = ad.get(\"title\") or ad.get(\"jobTitle\") or ad.get(\"headline\") or \"\"\n    # Long text fields that commonly contain responsibilities/skills\n    text_fields = []\n    for candidate in [\"description\", \"jobDescription\", \"summary\", \"adText\", \"details\"]:\n        if candidate in ad and ad[candidate]:\n            text_fields.append(str(ad[candidate]))\n    # Also include title and other short fields\n    text_fields.append(str(title))\n    job_text = \"\\n\".join([basic_clean_text(t) for t in text_fields if t])\n\n    result = {\n        \"id\": ad.get(\"id\") or ad.get(\"jobId\") or None,\n        \"title\": title,\n        \"raw_text\": job_text,\n        \"skills_baseline\": [],\n        \"responsibilities_baseline\": [],\n        \"requirements_baseline\": [],\n        \"skills_llm\": [],\n        \"responsibilities_llm\": [],\n        \"requirements_llm\": []\n    }\n\n    # Baseline extraction: simple keywords from job_text\n    baseline_keywords = simple_keyword_candidates(job_text, stopwords=set(SPACY_STOPWORDS) if spacy else None, max_ngram=2)\n    result[\"skills_baseline\"] = baseline_keywords[:20]\n\n    # KeyBERT if available\n    if use_keybert and KeyBERT is not None and SentenceTransformer is not None:\n        try:\n            kb = keybert_extract(job_text, top_n=15)\n            result[\"skills_baseline\"] = list(dict.fromkeys(kb + result[\"skills_baseline\"]))  # preserve order, de-dup\n        except Exception as e:\n            logger.debug(\"KeyBERT failed: %s\", e)\n\n    # LLM-based extraction (if API key is provided)\n    if llm_api_key:\n        try:\n            parsed = llm_extract(job_text, model=llm_model, api_key=llm_api_key)\n            result[\"skills_llm\"] = parsed.get(\"skills\", [])\n            result[\"responsibilities_llm\"] = parsed.get(\"responsibilities\", [])\n            result[\"requirements_llm\"] = parsed.get(\"requirements\", [])\n        except Exception as e:\n            logger.warning(\"LLM extraction failed for ad id %s: %s\", result.get(\"id\"), e)\n\n    # Simple heuristic for responsibilities/requirements from sentences (fallback)\n    sentences = re.split(r\"\\.|\\n|\\r\", job_text)\n    responsibilities = []\n    requirements = []\n    for s in sentences:\n        s_clean = s.strip()\n        if not s_clean or len(s_clean) < 20:\n            continue\n        # heuristics\n        if re.search(r\"\\b(responsibl|responsibilit|you will)\\b\", s_clean, re.I):\n            responsibilities.append(s_clean)\n        if re.search(r\"\\b(require|must|preferred|experience|degree|years)\\b\", s_clean, re.I):\n            requirements.append(s_clean)\n    result[\"responsibilities_baseline\"] = responsibilities[:10]\n    result[\"requirements_baseline\"] = requirements[:10]\n\n    # Embedding similarity optional: cluster or dedupe skills (not implemented fully here)\n    return result\n\n# -------------------------\n# Batch processing and evaluation\n# -------------------------\ndef process_ads(ads: List[Dict[str, Any]], output_path: str, llm_api_key: Optional[str] = None, max_records: Optional[int] = None):\n    \"\"\"\n    Process a list of job ads and write results to JSONL at output_path.\n    \"\"\"\n    outp = Path(output_path)\n    outp.parent.mkdir(parents=True, exist_ok=True)\n    count = 0\n    with open(outp, \"w\", encoding=\"utf-8\") as fout:\n        for ad in tqdm(ads[:max_records] if max_records else ads, desc=\"Processing ads\"):\n            try:\n                res = extract_from_ad(ad, llm_api_key=llm_api_key)\n                fout.write(json.dumps(res, ensure_ascii=False) + \"\\n\")\n                count += 1\n            except Exception as e:\n                logger.error(\"Failed to process ad: %s\", e)\n    logger.info(\"Wrote %d extracted records to %s\", count, outp)\n    return outp\n\ndef sample_and_manual_label(output_jsonl: str, sample_n: int = 10) -> List[Dict[str, Any]]:\n    \"\"\"\n    Utility to sample N extracted records for manual labeling.\n    This function will just return the sample list so the user can create labels in a notebook or file.\n    \"\"\"\n    results = []\n    with open(output_jsonl, \"r\", encoding=\"utf-8\") as f:\n        for i, line in enumerate(f):\n            if i >= sample_n:\n                break\n            results.append(json.loads(line))\n    return results\n\ndef evaluate_against_manual(predictions_jsonl: str, manual_labels: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"\n    Given a JSONL of predictions and a list of manual labels (dicts with the same structure),\n    compute simple precision/recall metrics for skills extraction.\n    This is a minimal example: exact-match / bag-of-words metrics.\n    \"\"\"\n    # Convert manual labels to dict by id (if ids present), else align by order.\n    preds = []\n    with open(predictions_jsonl, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            preds.append(json.loads(line))\n    n = min(len(preds), len(manual_labels))\n    total_precision = 0.0\n    total_recall = 0.0\n    for i in range(n):\n        p_skills = set(x.lower() for x in preds[i].get(\"skills_llm\", []) + preds[i].get(\"skills_baseline\", []))\n        g_skills = set(x.lower() for x in manual_labels[i].get(\"skills\", []))\n        if not p_skills and not g_skills:\n            continue\n        tp = len(p_skills & g_skills)\n        prec = tp / len(p_skills) if p_skills else 0.0\n        rec = tp / len(g_skills) if g_skills else 0.0\n        total_precision += prec\n        total_recall += rec\n    metrics = {\n        \"n_evaluated\": n,\n        \"precision_avg\": total_precision / n if n else 0.0,\n        \"recall_avg\": total_recall / n if n else 0.0\n    }\n    return metrics\n\n# -------------------------\n# Simple PowerPoint creator\n# -------------------------\ndef create_presentation(summary: Dict[str, Any], out_path: str = \"seek_solution_presentation.pptx\"):\n    \"\"\"\n    Create a simple PowerPoint that summarizes approach and findings.\n    This is intentionally minimal \u2014 adapt styling and content as needed.\n    \"\"\"\n    if Presentation is None:\n        logger.warning(\"python-pptx not installed; skipping PPTX generation.\")\n        return None\n    prs = Presentation()\n    # Title slide\n    slide_layout = prs.slide_layouts[0]\n    slide = prs.slides.add_slide(slide_layout)\n    title = slide.shapes.title\n    subtitle = slide.placeholders[1]\n    title.text = \"SEEK Job Metadata Extraction \u2014 Case Study\"\n    subtitle.text = \"Hybrid pipeline: Traditional NLP + Generative AI\\nAuto-generated summary\"\n\n    # Approach slide\n    slide = prs.slides.add_slide(prs.slide_layouts[1])\n    slide.shapes.title.text = \"Approach\"\n    body = slide.shapes.placeholders[1].text_frame\n    body.text = \"1) Data ingestion and cleaning\\n2) Baseline keyword extraction (KeyBERT / heuristics)\\n3) LLM-based structured extraction\\n4) Evaluation & scaling considerations\"\n\n    # Results slide\n    slide = prs.slides.add_slide(prs.slide_layouts[1])\n    slide.shapes.title.text = \"Sample Results\"\n    body = slide.shapes.placeholders[1].text_frame\n    sample_lines = summary.get(\"sample_lines\", [\"No sample\"])\n    for i, line in enumerate(sample_lines[:6]):\n        p = body.add_paragraph()\n        p.level = 1\n        p.text = line if isinstance(line, str) else json.dumps(line)[:200]\n\n    prs.save(out_path)\n    logger.info(\"Presentation saved to %s\", out_path)\n    return out_path\n\n# -------------------------\n# CLI and main\n# -------------------------\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"SEEK job metadata extraction pipeline\")\n    parser.add_argument(\"--input\", \"-i\", type=str, default=\"/mnt/data/random_rows.csv\", help=\"Path to input CSV/JSONL\")\n    parser.add_argument(\"--output\", \"-o\", type=str, default=\"./seek_extracted.jsonl\", help=\"Path to output JSONL\")\n    parser.add_argument(\"--max\", \"-m\", type=int, default=200, help=\"Max records to process (for testing)\")\n    parser.add_argument(\"--llm\", action=\"store_true\", help=\"Use LLM extraction (requires OPENAI_API_KEY in env)\")\n    parser.add_argument(\"--llm_model\", type=str, default=\"gpt-4o-mini\", help=\"LLM model name (OpenAI-compatible)\")\n    parser.add_argument(\"--pptx\", action=\"store_true\", help=\"Generate a simple PPTX summary\")\n    return parser.parse_args()\n\ndef main():\n    args = parse_args()\n    input_path = args.input\n    output_path = args.output\n    max_records = args.max\n\n    # Load data\n    ads = load_jsonl_csv(input_path)\n    logger.info(\"Loaded %d ads\", len(ads))\n\n    llm_key = None\n    if args.llm:\n        llm_key = os.environ.get(\"OPENAI_API_KEY\")\n        if not llm_key:\n            logger.error(\"LLM extraction requested but OPENAI_API_KEY not found in environment. Exiting.\")\n            return\n\n    processed_path = process_ads(ads, output_path, llm_api_key=llm_key, max_records=max_records)\n\n    # Sample for manual labeling and quick summary\n    sample = sample_and_manual_label(str(processed_path), sample_n=6)\n    summary = {\"n_processed\": len(ads[:max_records] if max_records else ads), \"sample_lines\": [s.get(\"title\") or s.get(\"raw_text\")[:200] for s in sample]}\n\n    if args.pptx:\n        pptx_path = create_presentation(summary, out_path=\"seek_solution_presentation.pptx\")\n        if pptx_path:\n            logger.info(\"PPTX created at %s\", pptx_path)\n\n    logger.info(\"Done. Output written to %s\", processed_path)\n\nif __name__ == \"__main__\":\n    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example usage\n",
        "Below is an example of how to run the pipeline from this notebook:\n",
        "\n",
        "```python\n",
        "ads = load_jsonl_csv('/mnt/data/random_rows.csv')\n",
        "results_path = process_ads(ads, './seek_extracted.jsonl', llm_api_key=os.getenv('OPENAI_API_KEY'), max_records=20)\n",
        "sample = sample_and_manual_label(results_path, sample_n=3)\n",
        "sample\n",
        "```\n",
        "\n",
        "You can modify the parameters and explore the structured outputs interactively."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}